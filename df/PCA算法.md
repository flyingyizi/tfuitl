# introduction

PCA（Principal Component Analysis）是一种常用的数据分析方法,用于数据降维度。降维的好处不仅仅是减少计算量，
也提供了一种显示多维数据的方法：显示间接的低维数据。

- 向量运算内积
  
  内积的几何意义为：设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度。

- 矩阵相乘的一种物理解释：

  两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，
  一个矩阵可以表示一种线性变换。

- 协方差矩阵及优化目标

  如果基的数量少于向量本身的维数，则可以达到降维的效果。如何选择基才是最优的，或者说，如果我们有一组N维向量，
  现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？

  直白的说希望投影后的投影值尽可能分散，这种分散程度可以用数学上的方差来表达。
  $$Var(a) = \frac{1}{m} \sum_{i=1}^{m} \left( a_i - u  \right)^2$$
  
  对已经均值为0的是：
  $$Var(a) = \frac{1}{m} \sum_{i=1}^{m} \left( a_i   \right)^2$$

  上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。

  方差解决了在一维上的选择目标，但对于多维，显然如果它们是线性相关的化，则还是投影在同一维度上，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。数学上可以用两个字段的**协方差**表示其相关性。
  $$Cov(a,b) = \frac{1}{m} \sum_{i=1}^{m} ( a_i - u_a) (b_i -u_b  )$$
  
  对已经均值为0的表达式是：
  $$Cov(a,b) = \frac{1}{m} \sum_{i=1}^{m} \left( a_i b_i  \right)$$

  我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。

- 协方差矩阵  

  在线性代数中有个神奇的存在,按照行组织数据（即列代表各个维度）得到X，$\frac{1}{m} X  X^T$结果矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵。

  总结来说就是：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\frac{1}{m} X  X^T$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。


## PCA算法

总结一下PCA的算法步骤：

设有m条n维数据。

- 1）将原始数据按列组成n行m列矩阵X

- 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

- 3）求出协方差矩阵$C=\frac{1}{m} X  X^T$

- 4）求出协方差矩阵的特征值及对应的特征向量

- 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵$P$

- 6）$Y=PX$ 即为降维到k维后的数据