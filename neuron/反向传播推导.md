论文[Learning representations by back-propagating errors](http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf)

[示例推导](http://www.cnblogs.com/charlotte77/p/5629865.html)

假设输入的样本数为$m$,输出是一个维度为$K$的向量, $g_{c,j}$代表第$c$个样本（index over cases）对第$j$个输出（index over output units）.总体误差（total error，$E$）定义为:
$$E=\frac{1}{2} \sum_c^m \sum_j^K (y_{j,c} -g_{j,c})^2$$

从上面定义可以看出，为了计算总误差，我们使用了训练集中的所有样本。

1/先看看总误差随着**某个输出单元预测值**的变化关系:
$$\frac{\partial E}{\partial g_{j,c}} = \frac{1}{2} \sum_c^m \frac{ \partial [ ...+(g_{j,c}-y_{j,c})^2 +... ] }{\partial g_{j,c}}$$

$$= \frac{1}{2} \sum_c^m [ 0+..+ \frac{\partial (g_{j,c}-y_{j,c})^2 }{\partial g_{j,c}}  +..+0 ] = \sum_c^m [ g_{j,c}-y_{j,c} ] $$
考虑到样本数量在我们的问题中是固定的，因此上面简写为：
$$\frac{\partial E}{\partial g_j} = [ g_j-y_j ]$$

从该结果可知，随着预测值的变化，总误差会根据预测值与真实值之间的差值，以同样的速率在变化。

2/下面再看看总误差随着**某个输出单元的总输入**的变化关系，同样由链式法则可以得到：
$$\frac{\partial E}{\partial z_j} = \frac{\partial E}{\partial g_j} \frac{\partial g_j}{\partial z_j}$$
其中$g_j=\frac{1}{1+e^{-z}}$, 并且$\frac{\partial g_j}{\partial z_j}=\frac{1}{ (1+e^{-z})^2 }=g_j(1-g_j)$,因此：
$$\frac{\partial E}{\partial z_j} = \frac{\partial E}{\partial g_j} g_j(1-g_j)=( g_j-y_j) g_j(1-g_j)$$

3/下面看看链接权重的变化对误差变化的影响.
假设$W_{i,j}$代表的是$l$层第$i$输出单元到$l+1$层$j$单元的权重.我们知道，对于某个输出单元的总输入$z_j$ 是隐藏层单元与权重的线性组合。

$$\frac{ \partial E }{ \partial W_{i,j} }= \frac{\partial E}{\partial z_j} . \frac{\partial z_j}{\partial W_{i,j}}$$